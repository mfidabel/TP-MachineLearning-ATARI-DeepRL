{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version +978d2ce)\n",
      "[Powered by Stella]\n",
      "Game console created:\n",
      "  ROM file:  /Users/mfidabel/Codigos/materias/ML/TP-ATARI/venv/lib/python3.8/site-packages/ale_py/roms/breakout.bin\n",
      "  Cart Name: Breakout - Breakaway IV (1978) (Atari)\n",
      "  Cart MD5:  f34f08e5eb96e500e851a80be3277a56\n",
      "  Display Format:  AUTO-DETECT ==> NTSC\n",
      "  ROM Size:        2048\n",
      "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
      "\n",
      "Running ROM file...\n",
      "Random seed is 1636902272\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ale_py import ALEInterface\n",
    "from ale_py.roms import Breakout\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from atari_wrappers import make_atari, wrap_deepmind\n",
    "\n",
    "ale = ALEInterface()\n",
    "ale.loadROM(Breakout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(42, 742738649)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "env = make_atari(\"BreakoutNoFrameskip-v4\")\n",
    "env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
    "env.seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "num_actions = 4\n",
    "\n",
    "def create_q_network():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(84, 84, 4, )),\n",
    "        tf.keras.layers.Conv2D(32, 8, strides=4, activation=\"relu\"),\n",
    "        tf.keras.layers.Conv2D(64, 4, strides=2, activation=\"relu\"),\n",
    "        tf.keras.layers.Conv2D(64, 3, strides=1, activation=\"relu\"),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(num_actions, activation=\"linear\")\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_q_network()\n",
    "model_target = create_q_network()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "loss_function = tf.keras.losses.Huber()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# No se usa al final\n",
    "def atari_preprocessing(state: np.array):\n",
    "    state = tf.convert_to_tensor(state)\n",
    "    state = tf.image.rgb_to_grayscale(state)\n",
    "    state = tf.image.resize(state, [110, 84])\n",
    "    state = state[17:-9,:,:]\n",
    "    return state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 0.29 at episode 288, frame count 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/vy/pn2_wb6d0mx0qtl7cx0xr9q80000gn/T/ipykernel_48522/3373136803.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     49\u001B[0m             \u001B[0;31m# Build the updated Q-values for the sampled future states\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m             \u001B[0;31m# Use the target model for stability\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m             \u001B[0mfuture_rewards\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel_target\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate_next_sample\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m             \u001B[0;31m# Q value = reward + discount factor * expected future reward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m             updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
      "\u001B[0;32m~/Codigos/materias/ML/TP-ATARI/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     65\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Codigos/materias/ML/TP-ATARI/venv/lib/python3.8/site-packages/keras/engine/training.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1783\u001B[0m       \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_predict_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1784\u001B[0m       \u001B[0mbatch_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1785\u001B[0;31m       \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miterator\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menumerate_epochs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Single epoch.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1786\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcatch_stop_iteration\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1787\u001B[0m           \u001B[0;32mfor\u001B[0m \u001B[0mstep\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Codigos/materias/ML/TP-ATARI/venv/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001B[0m in \u001B[0;36menumerate_epochs\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1193\u001B[0m     \u001B[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1194\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_truncate_execution_to_epoch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1195\u001B[0;31m       \u001B[0mdata_iterator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0miter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1196\u001B[0m       \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_initial_epoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_epochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1197\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_insufficient_data\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Set by `catch_stop_iteration`.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Codigos/materias/ML/TP-ATARI/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    488\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minside_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    489\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolocate_with\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_variant_tensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 490\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0miterator_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOwnedIterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    491\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    492\u001B[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001B[0;32m~/Codigos/materias/ML/TP-ATARI/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, dataset, components, element_spec)\u001B[0m\n\u001B[1;32m    724\u001B[0m             \u001B[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    725\u001B[0m             \"not be specified.\")\n\u001B[0;32m--> 726\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_iterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    727\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    728\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_next_call_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Codigos/materias/ML/TP-ATARI/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001B[0m in \u001B[0;36m_create_iterator\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    749\u001B[0m               \u001B[0moutput_types\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_flat_output_types\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    750\u001B[0m               output_shapes=self._flat_output_shapes))\n\u001B[0;32m--> 751\u001B[0;31m       \u001B[0mgen_dataset_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake_iterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mds_variant\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_iterator_resource\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    752\u001B[0m       \u001B[0;31m# Delete the resource when this object is deleted\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    753\u001B[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001B[0;32m~/Codigos/materias/ML/TP-ATARI/venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001B[0m in \u001B[0;36mmake_iterator\u001B[0;34m(dataset, iterator, name)\u001B[0m\n\u001B[1;32m   3235\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mtld\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3236\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3237\u001B[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0m\u001B[1;32m   3238\u001B[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001B[1;32m   3239\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    state = np.array(env.reset())\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Jugamos exploración\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Jugamos explotación\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Tomar mejor acción\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decaer probabilidad de tomar una acción aleatoria\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Aplicar la acción seleccionada\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Guardar al replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "         # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "\n",
    "    if running_reward > 40:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x1630a77c0>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_np = tf.constant(test)\n",
    "state = tf.image.rgb_to_grayscale(test_np)\n",
    "state = tf.image.resize(state, [110, 84])\n",
    "state = state[17:-9,:,:]\n",
    "print(state.shape)\n",
    "plt.imshow(state.numpy(), cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x162ea6130>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xd[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "EnvSpec(ALE/Breakout-v5)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}